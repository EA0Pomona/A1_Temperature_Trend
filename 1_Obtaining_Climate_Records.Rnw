\documentclass{article}
\usepackage{hyperref}
<<>>=

@

\title{Obtaining Climate Records}
\author{Marc Los Huertos}

\begin{document}
\maketitle

\section{Terrestrial Meteorological Data}

\subsection{Selected History of Meteorology}

In 1607, Galileo Galilei constructed a thermoscope, but it wasn't until 1654, that Ferdinando II de Medici established the first weather observing network, consisting of meteorological stations in Florence, Cutigliano, Vallombrosa, Bologna, Parma, Milan, Innsbruck, Osnabr√ºck, Paris and Warsaw. The collected data were sent to Florence at regular time intervals. In 1714, Gabriel Fahrenheit created a reliable scale for measuring temperature with a mercury-type thermometer. In 1742, Anders Celsius, a Swedish astronomer, proposed the ``centigrade" temperature scale, the predecessor of the current Celsius scale.

The arrival of the electrical telegraph in 1837 afforded, for the first time, a practical method for quickly gathering surface weather observations from a wide area. This data could be used to produce maps of the state of the atmosphere for a region near the Earth's surface and to study how these states evolved through time. 

To make frequent weather forecasts based on these data required a reliable network of observations, but it was not until 1849 that the Smithsonian Institution began to establish an observation network across the United States under the leadership of Joseph Henry. Similar observation networks were established in Europe at this time. 

\subsection{Purpose}
This document is a guide for getting started on the climate data analysis project. Here you will find: 
\begin{itemize}
\item different sources for climate data
\item a description of how to download/access the data
\end{itemize}
\subsection{Climate Data Online (CDO) system}

Climate Data Online (CDO) provides free access to the National Climate Data Center's archive of global historical weather and climate data in addition to station history information. These data include quality controlled daily, monthly, seasonal, and yearly measurements of temperature, precipitation, wind, and degree days as well as radar data and 30-year Climate Normals. 

There are several ways to obtain online climate data and the CDO-web interface is probably the easiest, using the following URL: \url{https://www.ncdc.noaa.gov/cdo-web/}. 

Climate data, such as temperature and precipitation are recorded at each station -- so these are the observational units we will generally be interested in for our analyses. Even if you don't have a location of interest nailed down, this interface can help you find sites and evaluate the length of their records. For the purpose of our project we reccomend picking a site that has data going back at least into the the 1940's as it might be difficult to come up with insightful observations about sites with less data. 

Below is a description of the steps to get data from the site:

\begin{description}
  \item[Follow Link] Click on the following link to reach the CDO website: \\ \url{https://www.ncdc.noaa.gov/cdo-web/} 
  \item[Click Browse Datasets] after clicking the icon you should see a list of available data sets by type
  \item[Expand Link] Daily Summaries
  \item[Select the Search Tool] additionally, you can use the mapping tool to evaluate which areas of the globe have available datasets but if you have a particular area in mind it might be easiest to search for it using the search tool
  \item[Specify Parameters] \hspace{30mm}
  
  \begin{itemize}
    \item ensure that ``Daily Summaries'' is selected from the dropdown menu in \textbf{Select Weather Observation Type/Dataset}
    \item in \textbf{Select Data Range} use the calendar tool change the start date to earliest date available (It should be January 1, 1763) and the end date to the most recently available date. This will enable you to see how far back the data record for the station goes.  
    \item in the drop down menu \textbf{Search For} select whether you want to search for climate data by country, state, county, zip code etc. Select the appropriate option for the search term that you will enter in the box below. \\ For example if you want to search for climate data in your hometown, you might consider searching by state or county and then typing in the name below
    \item in the \textbf{Enter a Search Term} box, type in the name of the county, state or other possible identifier of the place for which you would like to find a climate record.
    \end{itemize}

  \item[Start Search] Once you have initiated a search, the next window will give a list of records -- click in these records to see what they might be composed of. Then decide which station you will want to evaluate.
  \item[Add to Cart] Don't worry, there is no charge. 
  \item[View All items] Once you have added the climate record to your cart you should see an orange box pop up on the top left corner that says ``Cart (Free Data) 1 item." Mouse over the box after which you should have the option to click ``view all items".
  \item[Select Cart Options] in \textbf{Select the Output Format} be sure to click the button for \textbf{Custom GHCN-Daily CSV} to ensure that the data file is delivered as a .csv file (comma separated values)
  \item[Click `Continue' Button]
  \item[Station Detail \& Data Flag Options] Change to metric
  \item[Select Data Types] be sure to check the boxes for precipitation and temperature to include these in the data sets
  \item[Click `Continue' Button] NOTE: Common Errors:
Error -- *Text order size is 3,948 Station Years, which exceeds our capacity of 1,000 Station Years. Please select fewer stations/locations, or reduce the date range.

If you get this error, you need to reduce the number of stations selected or make the date range smaller.
  
  \item[Complete Order] Type in your email address and you will get a confirmation email relativley soon... the data may take up to a couple days to be delivered but are usually delivered within a couple hours.
\end{description}


\subsubsection{Evaluating the Data}

Once you get the data, we'll need to evaluate what we have and format it for our purposes. Often you'll find problems when you begin analyzing it, but it's good to understand the structure of the data before you run into problems. 

The CHCND documentation helps us to understand the data format, \url{https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf}.

Be sure you know the units of the data. The data might be in imperial or metric units and it's import to know before you get too far down the road. 

\section{Preparing CSV file(s)}

\subsection{Upload CSV Files into Appropriate Rstudio Directory} 
The first step to getting your data into R is to upload it from your computer into the the Rstudio server online but to simplify our work in the future first \textbf{locate the file} in your computer and rename it using the following convention: \\ 
\begin{center}
\textbf{yourname\_region\_data}
\end{center}
where you fill in your firstname and a short descriptor for the region to which your data pertain. \\

Now follow these steps to upload the data into the Rstudio server:

  \begin{enumerate}
  \item In Rstudio click on the folder entitled \textbf{``Climate\_Change\_Narratives"} using the \textit{Files} tab in the lower right navigation GUI (Window 4 from SOP 06)
  \item Navigate to the folder entitled \textbf{``Data"} 
  \item Next click on the correct folder for your class, it should correspond to the term in which you are enrolled in the EA 30 class. For example: SP17 if you're enrolled in EA 30 spring semester of 2017.
  \item Use the upload button to select a file from your computer to upload into the rstudio server
  \begin{figure}[h]
  \includegraphics[scale=0.25]{"Upload_button"}
  \end{figure}
  \item In the popup window select \textbf{Browse} and navigate to the climate data file you downloaded from the CDO website (SOP 84). It should have the new name you gave it above (yourname\_region\_data)
  \item Click \textbf{Open} and then click \textbf{OK}. 
  
  \end{enumerate}

Your data file is now uploaded to the Rstudio server! All this means is that the actual file is in your workspace on the server. This however, doesn't mean that the file has been uploaded (or ``pushed") to the repository on Github.com. If Marc has added you as a collaborator already now would be a good time to go through the process of pulling, committing and pusing with Git. Ask Marc to guide you through this in lan, and continue to the next section. 


%\subsection{Preprocessing CSV files}

%In most cases, we don't need to preprocess the csv files before uploading them. However, Mac users have been confronted with a host of problems that has something to do with how Macs format CSV files. If you have any problems with reading or uploading your csv file on a Mac speak with the instructor or TA. \footnote{I will update this when I try using a Mac for this.}


\section{Reading CSV Files into R}

Now that we got the file saved onto the Rstudio Server it's time to tell R to read the file so we can look at the data! This will require telling R to read the file. In order to do this we will use R commands. (remember these commands are to be typed in the R ``console") %One way to confirm this is to look at the Rstudio tab \textbf{`Environment'} in the top right window, where the file shoud be listed. If you don't see anything here it's because the file has not yet been loaded into R. 

 \subsection{Finding the data path to the file}
In order to tell R to read the file you also have to let it know where the file is (R is funny that way). We use an R command to help us find the specific data path to the file. \\
In the console type in the following and then follow these steps: 
\begin{verbatim}
file.choose()
\end{verbatim}

\begin{itemize} 
\item press enter on your keyboard
\item in the resulting popup window navigate to the your saved data file and \textbf{double click} on it
\item Look at the console window.. under the command that you just typed you should now see a data path. The data path should look something like this: ``/home/CAMPUS/im022012/Climate\_Change\_Narratives/Data/test\_data/Singapore\_ClimateData.csv"  It is the location of where your file is saved on the server. I suggest you truncate it to the path that after the home directory, i.e. ''Data/test\_data/Singapore\_ClimateData.csv''.

\item \textbf{Copy} the data path (including the parenthesis)
\end{itemize}

\subsection{Telling R to Read the Data File}

Now that we know where the data file is we will tell R to read it! In the console type in the following command and follow the next steps: 
\begin{verbatim}
read.csv()
\end{verbatim}

\begin{itemize}
\item This time before you press enter on the R command \textbf{Paste} in the data path you copied inbetween the parenthesis of the R command (be sure to include the parenthesis)
\item press enter
\item If your command worked you should see a bunch of text in your console window now. This means R read your file!
\end{itemize}

\subsection{Creating a Data Object}
So we made R read our file, however cool this might be, it is still not useful to us. In order to manipulate and do things with the data we need to tell R not only to read the file but also to store it in an object (such as a data frame) that we can manipulate. Follow these steps to tell R to read the file and store it in an object: \\

This time we will use the following R command\\
\begin{verbatim}
climate_data <- read.csv(''the/file/path'')
\end{verbatim}

\textbf{But be sure to use your actual data path!}

What this command is essentially doing is it's telling R to read the file and store it in an object called climate\_data ... if we really wanted to we could have actually named the object something else but climate\_data will suffice for now. \\ 

In R usually ``no news is good news" after you hit enter on a command. In other words, if you don't get red colored text describing an error, the program did something -- but now we need to figure out if it did something useful! Luckily there is a way to check if your command was successful in creating your object. All you have to do is check the \textbf{Environment} tab (in window 2). You should see your data object: climate\_data listed there. 



\section{Confirming the Proper Reading of the CSV file}
<<echo=FALSE, message=FALSE, results='hide'>>=
climate_data<- read.csv("Singapore_ClimateData.csv")
@

So we see that the data file is actually an object in R now. But what does it actually look like and what's in it? 
Since R was made to handle large data sets it tries not to overwhelm you with all the data at once but luckily the developers made a few commands to peak at the data to see if everthing is in order. We will try some of these commands now to look at few observations then we'll evaluate the structure of the dataframe and go on to finally plot the data!

\subsubsection{Viewing the 1st 6 Observations}
The following command allows you to view the first six observations of data in the data frame you created.
\begin{verbatim}
head(climate_data)
\end{verbatim}
Type it into the console and hit enter! You should get something that looks like the following

<<tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=65)>>=
head(climate_data)
@
You can see how the data is formatted into columns and rows like in Excel. 

\subsection{Evaluating the structure of the object}

Okay, so you now have created a data frame and taken a peak at it. But what if you want more information about it? Such as how many columns (a.k.a. variables) are in it. Or how many rows (a.k.a. observations) are in it. You can use the following command: \\
\begin{verbatim}
str(climate_data)
\end{verbatim} 
This function allow you to peer into the structure of the data frame giving output which describes how many variables they're are, how many observations of each as well as other useful information. Below you can see an example of the command. Checking the structure of the data frame is a another way to ensure that the data have been imported in a way that you expect.
<<tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=65)>>=
str(climate_data)
@
\subsection{Confirming the Column Names}
In R it isn't uncommon to want to manipulate a specific column of data within your data frame. Therefore it is useful to know what the names of the columns in your data frame are. The following command can be used to make R give you the names of the columns in your data frame. 

<<>>=
names(climate_data)
@

A data frame is essentially a set of ``vectors". Which themselves are like lists of numbers or text. You can think of each column as one of the vectors inside your data frame. If you want to access the data in one of the columns specifically, you can use the following command syntax:

\begin{verbatim}
nameofdataframe$columnname
\end{verbatim}

so for example, you can type in:

\begin{verbatim}
climate_data$TMAX
\end{verbatim}

And R will spit out the data in just that column. 

\section{Plotting the Data}

Now we will check the data by plotting it (Figure~\ref{fig:plotmissing}).

\begin{figure}
<<echo=T, fig=TRUE>>=
plot(TMAX~DATE, climate_data)

@
\caption{Some Caption.}
\label{fig:plotmissing}
\end{figure}

We find some rather odd low temperature values in the plot. We can find some of these with the \texttt{min()} function. This function works when there are no missing data, otherwise it will just report NA. 

<<>>=
min(climate_data$TMAX)
@

To get the function to cooperate if there are missing values, add 'na.rm=T' to the function:

<<>>=
min(climate_data$TMAX, na.rm=T)
@

\subsection{Re-assigning Missing Values to NAs}

In some of the stations, missing values were coded as  -9999? These are used for missing data. Historically, computers didn't have a lot of options for mixing numbers and letters in a variable type while R has some built in flexibility for this. So, to avoid leaving values blank (with all the ambiguous interpretations), the value -9999 is used to sybmolize missing values, since the number is unrealistic in the real world!

%Obviously, if we averaged the temperature with these values, we'd get a pretty inaccurate number (e.g. \Sexpr{round(mean(climate\_data$TMAX), 0)} versus \Sexpr{round(mean(climate\_data$TMAX[climate\_data$TMAX>-9999]), 0)}. Thus, we need to remove them!  

If your station used -9999 for missing values, we will replace these with NA, which R uses specifically to avoid accidently averaging arbitrary values that are representing missing values. 

How do we do this?
<<>>=
climate_data$TMAX[climate_data$TMAX==-9999] = NA
climate_data$TMIN[climate_data$TMIN==-9999] = NA
@


Okay, now we'll check again, but let's plot a just a few years, let's say five years (356 days * 5 years = 1825) or 1825 observations. 

<<>>=
plot(TMAX~DATE, climate_data[1:1835,], ty='l')
@

In the case of the Singapore data we have a new problem. What's wrong? It appears we have gaps in the data -- but we already removed our missing data, why are these big jumps in the data.

As it turns out stations might have one of three types of date formats -- and each of them are problematic. The first thing to note is that they are factors instead of begin formatted as a Date. You can check this with the following function:

<<>>=
str(climate_data)
@

In this case, we find that Date is a factor, which means that we will not be able to determine a trend because R has not recognized the data as a continuous variable. 


\subsection{Determine and Convert Date Format}

To create a new format, we have to complete a few steps. Unfortunately, date formats are one of the more obtuse aspects of R, but if you follow along, you should have success, even if you have no clue what you did. 
First, we convert the date to a string of character values. Next, we'll convert the strings to a data format. But we need to determine which type of data format we have from the following three choices then we can make the conversion:

\begin{description}
  \item[MM/DD/YYYY] This is a standard data for many US stations.
  
<<>>=
strDates <- as.character(climate_data$DATE)
head(strDates)

climate_data$NewDate <- as.Date(strDates, "%m/%d/Y%")
@

  \item[MM-DD-YYYY]
  
<<>>=
strDates <- as.character(climate_data$DATE)
head(strDates)

climate_data$NewDate <- as.Date(strDates, "%m-%d-Y%")
@

  \item[YYYYMMDD] As it turns out the problem is that with how the dates are specified. In particular, the Dec 31 to Jan 1 transition. Let's say that the data have a year change between 1913 and 1914. The date format in the NOAA data are YYYYMMDD, or year, month, and day with 4, 2, 2 digits, respectively. Thus, the last day of 1913 is 19131231 or Dec, 31, 1913. The next day is January 1st or 19140101. But when you plot these on the x-axis, the order of the values should be $19131231 \rightarrow 19131232 \rightarrow 19131233 \rightarrow 19131234$, etc but there is no 32nd, 33rd or 34th of December. Instead the dates go from  $19131231 \rightarrow 19140101$. We have lots of numbers that are skipped, but no coded as missing, but missing all the same. So, now we need to convert our dates to something more sensible. In R, that means creating a variable with a format that expects dates, thus doesn't plot numbers that are impossible dates!
  
<<>>=
strDates <- as.character(climate_data$DATE)
head(strDates)

climate_data$NewDate <- as.Date(strDates, "%Y%m%d")

climate_data$NewDate <- as.Date(strDates, "%Y%m%d", tryFormats = c("%Y-%m-%d", "%Y/%m/%d"), optional = FALSE)
@

  \item[Mixed Formats]... see Valentina's data!

Doesn't seem to work, I had to brut force it... will work on this next summer!


<<>>=
strDates <- as.character(climate_data$DATE)
head(strDates)

climate_data$NewDate <- as.Date(strDates, "%Y%m%d", tryFormats = c("%Y-%m-%d", "%Y/%m/%d"), optional = FALSE)
@

\end{description}



\subsection{Checking the New Dates}

<<>>=
plot(TMAX~NewDate, climate_data[1:1835,], ty='l')
@

%\subsection{Subset Sites}

%unique(import$STATION_NAME)
%Let's choose the FAIRPLEX NY US because the record is longer than the airport.

%LosAngeles <- subset(import, STATION_NAME=="LOS ANGELES DOWNTOWN USC CA US", select=c(STATION, STATION_NAME, DATE, NewDate, TMIN, TMAX, PRCP))


%plot(TMAX~NewDate, LosAngeles, ty='l')



to dump the average CO$_2$ concentrations readings onto your screen as a vector. You should see some ~627 observations, depending on how recent the data have been uploaded. So, the dollar symbol is used to drill into the data frame vectors.  And when you look at the \texttt{str()} function again, you will see these dollar signs again.

\section{Preparing Records for Analysis}

Our next task is to create a template to automatically run our code, so we can regenerate the pre-processed data within a Rmd file, which will then produce a figure. 

\subsection{Creating the Rmd as a blog template}

Start a new file -- File/New/Rmarkdown. Enter a draft title and make sure the author name is correct. Save the file in the your student folder. 

Knit to make sure it works. 

Now you will begin creating your blog by getting the commands you created before into an R block so that your data is available for the kniting process.

\begin{enumerate}
  \item Find the path for the csv file (\texttt{file.choose()}).
  \item Read the file into R (\texttt{read.csv(filepathfilename.csv})).
  \item Replace missing values with NA.
  \item Fix the date formats.
  \item Create a figure of Tmax versus time (\texttt{plot()}).
  \item Overlay the best fit line (\texttt{abline()}, \texttt{coef()}, and \texttt{lm()}).
  \item Go to SOP 90 to learn about how to analyze data.
  
\end{enumerate}


\end{document}